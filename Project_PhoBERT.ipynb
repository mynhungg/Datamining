{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UskTMptJmPuIWR_MhSadxElGB1FOPIEK",
      "authorship_tag": "ABX9TyP0C258wSLNGmkFk2/KXHxx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mynhungg/Datamining/blob/main/Project_PhoBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import thư viện\n",
        "\n"
      ],
      "metadata": {
        "id": "oE96HGkHDfUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "pry2ytalDk-1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##1.   Load model BERT\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i2exXVQJxUiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "path = \"/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC\"\n",
        "os.chdir(path)\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8k2L-iCxUNi",
        "outputId": "417a5e19-30fa-4457-b5ec-0d8ff0f9ba0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "PhoBERT_base_fairseq  PhoBERT_base_fairseq.tar.gz  vncorenlp  VNTC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Cài package\n",
        "*   fairseq\n",
        "*   fastBPE\n",
        "*   vncorenlp\n",
        "*   transformers\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FBipNT9pEHIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install fairseq\n",
        "!pip3 install fastbpe\n",
        "!pip3 install vncorenlp\n",
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "HQd_I_nFEO8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f445c661-0867-4109-dc8e-7386f82d181c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.34)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.65.0)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.22.4)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (16.0.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11170725 sha256=43fdf13307eeada495637e955235ff5a9f53ee4ff1265b229cd295c7a331c8af\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=b294790e03e2906e79193b5e7ec79037010cf7559434b6b7adeca67e928269f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastbpe\n",
            "  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fastbpe\n",
            "  Building wheel for fastbpe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastbpe: filename=fastBPE-0.1.0-cp310-cp310-linux_x86_64.whl size=767947 sha256=95ac94484847dfb5582b44024493c99702383c534ac42959df631cd8aa789137\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/5d/b9/4b8897941ebc9e8c6cc3f3ffd3ea5115731754269205098754\n",
            "Successfully built fastbpe\n",
            "Installing collected packages: fastbpe\n",
            "Successfully installed fastbpe-0.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=983285cd1cedadd23c47f4d66241e6fd32c9d14f42384802c6a65a86c42a4f56\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.   Download model pretrain từ [PhoBERT](https://github.com/VinAIResearch/PhoBERT)\n",
        "\n",
        "Project sử dụng pretrain model BERT base được huấn luyện từ package fairseq.\n"
      ],
      "metadata": {
        "id": "JoFLSFvREpSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
        "!tar -xzvf PhoBERT_base_fairseq.tar.gz"
      ],
      "metadata": {
        "id": "Ps6uTKkFE9Ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd43376-a30c-4a49-f77a-783896bd16cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-23 02:03:01--  https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 18.154.132.105, 18.154.132.37, 18.154.132.98, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|18.154.132.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1243308020 (1.2G) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_fairseq.tar.gz.1’\n",
            "\n",
            "PhoBERT_base_fairse 100%[===================>]   1.16G  36.3MB/s    in 30s     \n",
            "\n",
            "2023-05-23 02:03:31 (40.0 MB/s) - ‘PhoBERT_base_fairseq.tar.gz.1’ saved [1243308020/1243308020]\n",
            "\n",
            "PhoBERT_base_fairseq/\n",
            "PhoBERT_base_fairseq/bpe.codes\n",
            "PhoBERT_base_fairseq/model.pt\n",
            "PhoBERT_base_fairseq/dict.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi download và giải nén pretrain file chúng ta sẽ kiểm tra thấy bên trong folder sẽ bao gồm 3 files đó là `bpe.codes, dict.txt, model.pt` có tác dụng như sau:\n",
        "\n",
        "* bpe.codes: Là BPE token mà mô hình đã áp dụng để mã hóa văn bản sang index.\n",
        "\n",
        "* dict.txt: Từ điển của bộ dữ liệu huấn luyện.\n",
        "\n",
        "* model.pt: File lưu trữ của mô hình trên pytorch."
      ],
      "metadata": {
        "id": "W7vB0Ri1FcQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls PhoBERT_base_fairseq"
      ],
      "metadata": {
        "id": "rfRM6lwEETBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e759daa-3ca4-4777-a865-166c28fa8122"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bpe.codes  dict.txt  model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load model pretrain PhoBERT"
      ],
      "metadata": {
        "id": "zfhtNdHPFnH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT.eval()  # disable dropout (or leave in train mode to finetune"
      ],
      "metadata": {
        "id": "2z_0VK8PFzBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8983185b-be16-4e9d-8254-96a76e5629ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1042301B [00:00, 31405860.97B/s]\n",
            "456318B [00:00, 7003342.30B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (encoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(64001, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(258, 768, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x TransformerEncoderLayerBase(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta có thể thấy kiến trúc RoBERTa đã giữa lại 12 block sub-layers là các multi-head attention ở phase Encoder và thêm một linear projection layer ở cuối để tạo ra một embedding cho từ hiện tại."
      ],
      "metadata": {
        "id": "Ic6PN1x1Fuka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Áp dụng BPE tokenize trong BERT**"
      ],
      "metadata": {
        "id": "2vbo_zvbF9pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install fairseq\n",
        "!pip3 install fastBPE"
      ],
      "metadata": {
        "id": "96AYiQE7Gi8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6300987-f2a2-4d6a-adc8-98124f0f2ddf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.34)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.65.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.7.3)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.22.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (16.0.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.10/dist-packages (0.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model pretrain `RoBERTa`."
      ],
      "metadata": {
        "id": "Pe4LlyVcGle4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT.eval()  # disable dropout (or leave in train mode to finetune"
      ],
      "metadata": {
        "id": "02CebeW2GplJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc579503-2ac2-411a-89dd-3e626fe5f003"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (encoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(64001, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(258, 768, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x TransformerEncoderLayerBase(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khai báo bpe tokenizer và thực hiện token."
      ],
      "metadata": {
        "id": "loemyMm4GuUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "tokens = phoBERT.encode('Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh')\n",
        "print('tokens list : ', tokens)\n",
        "# Decode ngược lại thành câu từ chuỗi index token\n",
        "phoBERT.decode(tokens)  # 'Hello world!'"
      ],
      "metadata": {
        "id": "LuwKZO3LGw--",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a1f018d2-eca6-4df6-97fb-40b2a705de00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens list :  tensor([    0, 11623, 31433,   453, 44334,  2080,  5922,    57,   934,  8181,\n",
            "        31686,  3078,     2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down load package VnCoreNLP để tokenize các câu văn."
      ],
      "metadata": {
        "id": "IJmVa_48Gv6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "metadata": {
        "id": "SNtWrjCeG77a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bed941-390d-4d95-f478-200876fbc19b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-23 02:06:14--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  58.8MB/s    in 0.4s    \n",
            "\n",
            "2023-05-23 02:06:16 (58.8 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2023-05-23 02:06:16--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-05-23 02:06:16 (18.4 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2023-05-23 02:06:16--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-05-23 02:06:16 (9.23 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gỉa sử chúng ta có câu gốc là `Tôn Ngộ Không phò Đường Tăng đi thỉnh kinh tại Tây Trúc`. Từ được ẩn đi trong câu là `phò` sẽ được thay thế bằng token `<mask>`."
      ],
      "metadata": {
        "id": "HWv4Y67dHhM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "\n",
        "text = 'Tôn Ngộ Không phò Đường Tăng đi thỉnh kinh tại Tây Trúc'\n",
        "text_masked = 'Học sinh được  <mask> do dịch covid-19'\n",
        "# Tokenize câu gốc và thay từ phò bằng <mask>\n",
        "words = rdrsegmenter.tokenize(text)[0]\n",
        "for i, token in enumerate(words):\n",
        "  if token == 'phò':\n",
        "    words[i] = ' <mask>'\n",
        "text_masked_tok = ' '.join(words)\n",
        "print('text_masked_tok: \\n', text_masked_tok)"
      ],
      "metadata": {
        "id": "XvyS1c2sHh_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8807d790-a3ab-4b2b-df8b-abb369f15ce7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_masked_tok: \n",
            " Tôn_Ngộ_Không  <mask> Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tìm ra top 10 từ thích hợp nhất cho vị trí `<mask>` tại câu trên."
      ],
      "metadata": {
        "id": "BpfdmhujHkag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RLPbdhtdIMPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE  \n",
        "from fairseq import options  \n",
        "import numpy as np\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "\n",
        "# Filling marks  \n",
        "topk_filled_outputs = phoBERT.fill_mask(text_masked_tok, topk=10) \n",
        "topk_probs = [item[1] for item in topk_filled_outputs]\n",
        "print('Total probability: ', np.sum(topk_probs))\n",
        "print('Input sequence: ', text_masked_tok)\n",
        "print('Top 10 in mask: ')\n",
        "for i, output in enumerate(topk_filled_outputs): \n",
        "  print(output[0])"
      ],
      "metadata": {
        "id": "yvbV1GtKHk59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47dac28-2d93-436e-b324-697ec8c7e1de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total probability:  0.8735209610313177\n",
            "Input sequence:  Tôn_Ngộ_Không  <mask> Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Top 10 in mask: \n",
            "Tôn_Ngộ_Không và Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không đưa Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không cõng Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không hộ_tống Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không cùng Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không chở Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không theo Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không dẫn Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không , Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n",
            "Tôn_Ngộ_Không tháp_tùng Đường Tăng đi thỉnh_kinh tại Tây_Trúc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Extract feature cho các từ\n",
        "\n",
        "Chúng ta có thể tìm ra được các véc tơ embedding cho từng từ trong câu từ mô hình BERT như sau:"
      ],
      "metadata": {
        "id": "7Wmrmj1GHsTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "doc = phoBERT.extract_features_aligned_to_words('học_sinh cấp 3 được đến trường sau nghỉ dịch covid')\n",
        "\n",
        "for tok in doc:\n",
        "    print('{:10}{} (...) {}'.format(str(tok), tok.vector[:5], tok.vector.size()))"
      ],
      "metadata": {
        "id": "FsNPoeCSHqCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "6c3f349e-b2c6-43db-a1d7-47f4cbbc4129"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-82ce823974fc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mphoBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfastBPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Incorporate the BPE encoder into PhoBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphoBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features_aligned_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'học_sinh cấp 3 được đến trường sau nghỉ dịch covid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features_aligned_to_words\u001b[0;34m(self, sentence, return_all_hiddens)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# tokenize both with GPT-2 BPE and spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairseq/models/roberta/alignment_utils.py\u001b[0m in \u001b[0;36mspacy_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please install spacy with: pip install spacy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'EnglishDefaults' has no attribute 'create_tokenizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Bài toán classification"
      ],
      "metadata": {
        "id": "UXj__KAnIN5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2. Dữ liệu\n",
        "\n",
        "sử dụng dữ liệu [VNTC](https://github.com/duyvuleo/VNTC.git) với các bài báo đã được sắp xếp theo 10 topics. Bộ dữ liệu bao gồm 33 nghìn bài báo trên tập train và 50 nghìn bài báo trên tập test có phân bố số lượng theo topics như sau:\n",
        "\n",
        "![](https://imgur.com/1lDTdC1.png)"
      ],
      "metadata": {
        "id": "P6YIYujoIRw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.2.1. Đọc và lưu dữ liệu"
      ],
      "metadata": {
        "id": "t_NPXfX2IYkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/gdrive/MyDrive/Colab\\ Notebooks/PhoBERT_VNTC/VNTC/.git/hooks/post-checkout\n"
      ],
      "metadata": {
        "id": "0B4CIrquLfUI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/duyvuleo/VNTC.git\n",
        "!ls VNTC/Data/10Topics/Ver1.1"
      ],
      "metadata": {
        "id": "iU-VqmAYIcP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract file dataset"
      ],
      "metadata": {
        "id": "VHWU-45D3-FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install patool\n",
        "import patoolib\n",
        "# Specify the path to the RAR file\n",
        "rar_file_path = '/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1/Train_Full.rar'\n",
        "\n",
        "# Specify the destination directory for extraction\n",
        "destination_dir = '/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1'\n",
        "\n",
        "# Extract the RAR file\n",
        "patoolib.extract_archive(rar_file_path, outdir=destination_dir)"
      ],
      "metadata": {
        "id": "BpCcXOiWzUfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "b100e54a-0a69-40c2-a98a-ac58b9ea1a92"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n",
            "patool: Extracting /content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1/Train_Full.rar ...\n",
            "patool: running /usr/bin/unrar x -- \"/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1/Train_Full.rar\"\n",
            "patool:     with cwd='/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PatoolError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPatoolError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e356732d5ffa>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract the RAR file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpatoolib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrar_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdestination_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36mextract_archive\u001b[0;34m(archive, verbosity, outdir, program, interactive)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbosity\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting %s ...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36m_extract_archive\u001b[0;34m(archive, verbosity, interactive, outdir, program, format, compression)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# already handled the command (eg. when it's a builtin Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mrun_archive_cmdlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_cleanup_outdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanup_outdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36mrun_archive_cmdlist\u001b[0;34m(archive_cmdlist, verbosity)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchive_cmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_checked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrunkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/util.py\u001b[0m in \u001b[0;36mrun_checked\u001b[0;34m(cmd, ret_ok, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Command `%s' returned non-zero exit status %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mPatoolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPatoolError\u001b[0m: Command `['/usr/bin/unrar', 'x', '--', '/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1/Train_Full.rar']' returned non-zero exit status 255"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the RAR file\n",
        "rar_file_path = '/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1/Test_Full.rar'\n",
        "\n",
        "# Specify the destination directory for extraction\n",
        "destination_dir = '/content/gdrive/MyDrive/Colab Notebooks/PhoBERT_VNTC/VNTC/Data/10Topics/Ver1.1'\n",
        "\n",
        "# Extract the RAR file\n",
        "patoolib.extract_archive(rar_file_path, outdir=destination_dir)"
      ],
      "metadata": {
        "id": "BAy2qZiT5gvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi đã download dữ liệu về, chúng ta sẽ đọc và lưu các bài báo vào những list chứa nội dung và nhãn tương ứng theo 2 folders train và test."
      ],
      "metadata": {
        "id": "AS2FYMBnIbwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob2\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_path = 'Train_Full/*/*.txt'\n",
        "test_path = 'Test_Full/*/*.txt'\n",
        "\n",
        "# Hàm đọc file txt\n",
        "def read_txt(path):\n",
        "  with open(path, 'r', encoding='utf-16') as f:\n",
        "    data = f.read()\n",
        "  return data\n",
        "\n",
        "# Hàm tạo dữ liệu huấn luyện cho tập train và test\n",
        "def make_data(path):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  for file_path in tqdm(glob2.glob(train_path)):\n",
        "    try:\n",
        "      content = read_txt(file_path)\n",
        "      label = file_path.split('/')[1]\n",
        "      texts.append(content)\n",
        "      labels.append(label)\n",
        "    except:\n",
        "      next\n",
        "  return texts, labels\n",
        "\n",
        "text_train, label_train = make_data(train_path)\n",
        "text_test, label_test = make_data(test_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNnrjdQJY6Wq",
        "outputId": "35d96727-1aaf-4563-a0f8-11ecc724e799"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tạo các hàm lưu trữ lại các list nội dung và nhãn và load lại cho lượt huấn luyện sau."
      ],
      "metadata": {
        "id": "oqekoMSpIoy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def _save_pkl(path, obj):\n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump(obj, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "hRfUAZ8SIqAK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_pkl(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    obj = pickle.load(f)\n",
        "  return obj\n"
      ],
      "metadata": {
        "id": "apfxtXJ--6Ih"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lưu lại các files\n",
        "_save_pkl('text_train.pkl', text_train)\n",
        "_save_pkl('label_train.pkl', label_train)\n",
        "_save_pkl('text_test.pkl', text_test)\n",
        "_save_pkl('label_test.pkl', label_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "znzItcjM-8Py",
        "outputId": "850e8589-639e-4177-9a52-06b080b54d1f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2a169b4668cf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lưu lại các files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_save_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_train.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_save_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_train.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_save_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_test.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_save_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label_test.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('text content:\\n', text_train[0])\n",
        "print('label:\\n', label_train[0])"
      ],
      "metadata": {
        "id": "7h0Pmup6IrJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8.2.2. Tokenize nội dung\n",
        "\n",
        "Tiếp theo ta sẽ tokenize các câu văn sang chuỗi index và padding câu văn về cùng một độ dài."
      ],
      "metadata": {
        "id": "CuwXUwc-JQWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 500\n",
        "\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  '''\n",
        "  lines: list các văn bản input\n",
        "  vocab: từ điển dùng để encoding subwords\n",
        "  bpe: \n",
        "  '''\n",
        "  # Khởi tạo ma trận output\n",
        "  outputs = np.zeros((len(lines), max_sequence_length)) # --> shape (number_lines, max_seq_len)\n",
        "  # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for idx, row in tqdm(enumerate(lines), total=len(lines)): \n",
        "    # Mã hóa subwords theo byte pair encoding(bpe)\n",
        "    subwords = bpe.encode('<s> '+ row +' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    # Truncate input nếu độ dài vượt quá max_seq_len\n",
        "    if len(input_ids) > max_sequence_length: \n",
        "      input_ids = input_ids[:max_sequence_length] \n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      # Padding nếu độ dài câu chưa bằng max_seq_len\n",
        "      input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n",
        "    \n",
        "    outputs[idx,:] = np.array(input_ids)\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "XkyGDtoAImBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3. Tokenize Input và output\n",
        "\n",
        "* Chuẩn bị X input: Tokenize nội dung các văn bản sang chuỗi indices.\n",
        "\n",
        "* Chuẩn bị y output: Encoding các label output thành indices đánh dấu số thứ tự của văn bản."
      ],
      "metadata": {
        "id": "kTC8LVPKIZJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "max_sequence_length = 256\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  '''\n",
        "  lines: list các văn bản input\n",
        "  vocab: từ điển dùng để encoding subwords\n",
        "  bpe: \n",
        "  '''\n",
        "  # Khởi tạo ma trận output\n",
        "  outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n",
        "  # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for idx, row in tqdm(enumerate(lines), total=len(lines)): \n",
        "    # Mã hóa subwords theo byte pair encoding(bpe)\n",
        "    subwords = bpe.encode('<s> '+ row +' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    # Truncate input nếu độ dài vượt quá max_seq_len\n",
        "    if len(input_ids) > max_sequence_length: \n",
        "      input_ids = input_ids[:max_sequence_length] \n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      # Padding nếu độ dài câu chưa bằng max_seq_len\n",
        "      input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n",
        "    \n",
        "    outputs[idx,:] = np.array(input_ids)\n",
        "  return outputs\n",
        "\n",
        "# Load the dictionary  \n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"PhoBERT_base_transformers/dict.txt\")\n",
        "\n",
        "\n",
        "# Test encode lines\n",
        "lines = ['Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19', 'số lượng ca nhiễm bệnh đã giảm bắt đầu từ tháng 5 nhờ biện pháp mạnh tay']\n",
        "[x1, x2] = convert_lines(lines, vocab, phoBERT.bpe)\n",
        "print('x1 tensor encode: {}, shape: {}'.format(x1[:10], x1.size))\n",
        "print('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])"
      ],
      "metadata": {
        "id": "JWXKXBUqsFHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "max_sequence_length = 256\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  '''\n",
        "  lines: list các văn bản input\n",
        "  vocab: từ điển dùng để encoding subwords\n",
        "  bpe: \n",
        "  '''\n",
        "  # Khởi tạo ma trận output\n",
        "  outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n",
        "  # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for idx, row in tqdm(enumerate(lines), total=len(lines)): \n",
        "    # Mã hóa subwords theo byte pair encoding(bpe)\n",
        "    subwords = bpe.encode('<s> '+ row +' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    # Truncate input nếu độ dài vượt quá max_seq_len\n",
        "    if len(input_ids) > max_sequence_length: \n",
        "      input_ids = input_ids[:max_sequence_length] \n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      # Padding nếu độ dài câu chưa bằng max_seq_len\n",
        "      input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n",
        "    \n",
        "    outputs[idx,:] = np.array(input_ids)\n",
        "  return outputs\n",
        "\n",
        "# Load the dictionary  \n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"PhoBERT_base_transformers/dict.txt\")\n",
        "\n",
        "\n",
        "# Test encode lines\n",
        "lines = ['Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19', 'số lượng ca nhiễm bệnh đã giảm bắt đầu từ tháng 5 nhờ biện pháp mạnh tay']\n",
        "[x1, x2] = convert_lines(lines, vocab, phoBERT_cls.bpe)\n",
        "print('x1 tensor encode: {}, shape: {}'.format(x1[:10], x1.size))\n",
        "print('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])"
      ],
      "metadata": {
        "id": "LK9gtIucsGHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Như vậy ta thấy rằng các câu văn đã được encode về token index. Từ token index có thể decode ngược trở lại thành câu input sau khi đã thêm các token đặc biệt đánh dấu vị trí bắt dầu: `<s>`, kết thúc: `</s>` câu và các vị trí nằm ngoài câu: `<pad>`. Ta sẽ token toàn bộ câu input sang index như sau:"
      ],
      "metadata": {
        "id": "VwrQRavKsudw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = convert_lines(text_train, vocab, phoBERT_cls.bpe)\n",
        "print('X shape: ', X.shape)"
      ],
      "metadata": {
        "id": "o-ua7CMusvFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau cùng ta thu được các chuỗi index có kích thước là 256, bằng với kích thước của các câu sau khi đã padding. Tiếp theo ta tạo output y bằng index cho các nhãn của câu."
      ],
      "metadata": {
        "id": "Mf4aCsSLszrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "lb.fit(label_train)\n",
        "y = lb.fit_transform(label_train)\n",
        "print(lb.classes_)\n",
        "print('Top 5 classes indices: ', y[:5])"
      ],
      "metadata": {
        "id": "spUJhkMNsyfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lưu lại dữ liệu $\\mathbf{X}$ và $\\mathbf{y}$"
      ],
      "metadata": {
        "id": "swYHwZA_s4aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save dữ liệu\n",
        "_save_pkl('PhoBERT_pretrain/X1.pkl', X)\n",
        "_save_pkl('PhoBERT_pretrain/y1.pkl', y)\n",
        "_save_pkl('PhoBERT_pretrain/labelEncoder1.pkl', lb)\n",
        "\n",
        "# Load lại dữ liệu\n",
        "X = _load_pkl('PhoBERT_pretrain/X1.pkl')\n",
        "y = _load_pkl('PhoBERT_pretrain/y1.pkl')\n",
        "\n",
        "print('length of X: ', len(X))\n",
        "print('length of y: ', len(y))"
      ],
      "metadata": {
        "id": "NcjvnT8Ts3WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4. Load model BERT"
      ],
      "metadata": {
        "id": "S-BOhZVss7ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "\n",
        "phoBERT_cls = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n",
        "\n",
        "# Load BPE\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "\n",
        "# Add header cho classification với số lượng classes = 10\n",
        "phoBERT_cls.register_classification_head('new_task', num_classes=10)\n",
        "tokens = 'Học_sinh được nghỉ học bắt đầu từ tháng 3 do ảnh hưởng của dịch covid-19'\n",
        "token_idxs = phoBERT_cls.encode(tokens)\n",
        "logprobs = phoBERT_cls.predict('new_task', token_idxs)  # tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=<LogSoftmaxBackward>)\n",
        "logprobs"
      ],
      "metadata": {
        "id": "mTsE43bUs8HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng hàm đánh giá mô hình theo 2 metric là accuracy và f1_score."
      ],
      "metadata": {
        "id": "jIi0ll-xtA9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5. Huấn luyện model"
      ],
      "metadata": {
        "id": "rBSIAnLHs-67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate(logits, targets):\n",
        "    \"\"\"\n",
        "    Đánh giá model sử dụng accuracy và f1 scores.\n",
        "    Args:\n",
        "        logits (B,C): torch.LongTensor. giá trị predicted logit cho class output.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        acc (float): the accuracy score\n",
        "        f1 (float): the f1 score\n",
        "    \"\"\"\n",
        "    # Tính accuracy score và f1_score\n",
        "    logits = logits.detach().cpu().numpy()    \n",
        "    y_pred = np.argmax(logits, axis = 1)\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    f1 = f1_score(targets, y_pred, average='weighted')\n",
        "    acc = accuracy_score(targets, y_pred)\n",
        "    return acc, f1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logits = torch.tensor([[0.1, 0.2, 0.7],\n",
        "                       [0.4, 0.1, 0.5],\n",
        "                       [0.1, 0.2, 0.7]]).to(device)\n",
        "targets = torch.tensor([1, 2, 2]).to(device)\n",
        "evaluate(logits, targets)"
      ],
      "metadata": {
        "id": "EcQ1SbWNs_Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_loader, model, device):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    f1s = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in valid_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model.predict('new_task', x_batch)\n",
        "            logits = torch.exp(outputs)\n",
        "            acc, f1 = evaluate(logits, y_batch)\n",
        "            accs.append(acc)\n",
        "            f1s.append(f1)\n",
        "    \n",
        "    mean_acc = np.mean(accs)\n",
        "    mean_f1 = np.mean(f1s)\n",
        "    return mean_acc, mean_f1"
      ],
      "metadata": {
        "id": "YbJbmoNOtDjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm huấn luyện mô hình trên từng epoch."
      ],
      "metadata": {
        "id": "SI4iUd6btDLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainOnEpoch(train_loader, model, optimizer, epoch, num_epochs, criteria, device, log_aggr = 100):\n",
        "    model.train()\n",
        "    sum_epoch_loss = 0\n",
        "    sum_acc = 0\n",
        "    sum_f1 = 0\n",
        "    start = time.time()\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model.predict('new_task', x_batch)\n",
        "      logits = torch.exp(y_pred)\n",
        "      acc, f1 = evaluate(logits, y_batch)\n",
        "      loss = criteria(y_pred, y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_val = loss.item()\n",
        "      sum_epoch_loss += loss_val\n",
        "      sum_acc += acc\n",
        "      sum_f1 += f1\n",
        "      iter_num = epoch * len(train_loader) + i + 1\n",
        "\n",
        "      if i % log_aggr == 0:\n",
        "            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f),  avg acc: %.4f, avg f1: %.4f, (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),  sum_acc/(i+1), sum_f1/(i+1),\n",
        "                  len(x_batch) / (time.time() - start)))\n",
        "      start = time.time()  "
      ],
      "metadata": {
        "id": "eyEEDWtxtF3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quá trình huấn luyện một model classification trên pytorch sẽ bao gồm những bước chính sau đây:\n",
        "\n",
        "* Khởi tạo DataLoader để quản lý dữ liệu đưa vào huấn luyện và thẩm định.\n",
        "\n",
        "* Thiết lập kiến trúc mô hình.\n",
        "\n",
        "* Khai báo hàm loss function.\n",
        "\n",
        "* Phương pháp optimization giúp tối ưu loss function.\n",
        "\n",
        "* Huấn luyện mô hình qua các epochs.\n",
        "\n",
        "Bên dưới chúng ta sẽ lần lượt thực hiện các bước trên."
      ],
      "metadata": {
        "id": "sqiIERLDtLW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "from transformers.modeling_utils import * \n",
        "from transformers import *\n",
        "\n",
        "# Khởi tạo argument\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 6\n",
        "ACCUMULATION_STEPS = 5\n",
        "FOLD = 4\n",
        "LR = 0.0001\n",
        "LR_DC_STEP = 80 \n",
        "LR_DC = 0.1\n",
        "CUR_DIR = os.path.dirname(os.getcwd())\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "FOLD = 4\n",
        "CKPT_PATH2 = 'model_ckpt2'\n",
        "\n",
        "if not os.path.exists(CKPT_PATH2):\n",
        "    os.mkdir(CKPT_PATH2)\n",
        "\n",
        "# Khởi tạo DataLoader\n",
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(X, y))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(splits):\n",
        "    best_score = 0\n",
        "    if fold != FOLD:\n",
        "        continue\n",
        "    print(\"Training for fold {}\".format(fold))\n",
        "    \n",
        "    # Create dataset\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X[train_idx],dtype=torch.long), torch.tensor(y[train_idx],dtype=torch.long))\n",
        "    valid_dataset = torch.utils.data.TensorDataset(torch.tensor(X[val_idx],dtype=torch.long), torch.tensor(y[val_idx],dtype=torch.long))\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Khởi tạo model:\n",
        "    MODEL_LAST_CKPT = os.path.join(CKPT_PATH2, 'latest_checkpoint.pth.tar')\n",
        "    if os.path.exists(MODEL_LAST_CKPT):\n",
        "      print('Load checkpoint model!')\n",
        "      phoBERT_cls = torch.load(MODEL_LAST_CKPT)\n",
        "    else:\n",
        "      print('Load model pretrained!')\n",
        "      # Load the model in fairseq\n",
        "      from fairseq.models.roberta import RobertaModel\n",
        "      from fairseq.data.encoders.fastbpe import fastBPE\n",
        "      from fairseq.data import Dictionary\n",
        "\n",
        "      phoBERT_cls = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "      phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n",
        "\n",
        "      # # Load BPE\n",
        "      # class BPE():\n",
        "      #   bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "      # args = BPE()\n",
        "      # phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "\n",
        "      # Add header cho classification với số lượng classes = 10\n",
        "      phoBERT_cls.register_classification_head('new_task', num_classes=10)\n",
        "      \n",
        "    ## Load BPE\n",
        "    print('Load BPE')\n",
        "    class BPE():\n",
        "      bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "    args = BPE()\n",
        "    phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "    phoBERT_cls.to(DEVICE)\n",
        "\n",
        "    # Khởi tạo optimizer và scheduler, criteria\n",
        "    print('Init Optimizer, scheduler, criteria')\n",
        "    param_optimizer = list(phoBERT_cls.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/BATCH_SIZE/ACCUMULATION_STEPS)\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=LR, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # scheduler với linear warmup\n",
        "    scheduler0 = get_constant_schedule(optimizer)  # scheduler với hằng số\n",
        "    # optimizer = optim.Adam(phoBERT_cls.parameters(), LR)\n",
        "    criteria = nn.NLLLoss()\n",
        "    # scheduler = StepLR(optimizer, step_size = LR_DC_STEP, gamma = LR_DC)\n",
        "    avg_loss = 0.\n",
        "    avg_accuracy = 0.\n",
        "    frozen = True\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        # warm up tại epoch đầu tiên, sau epoch đầu sẽ phá băng các layers\n",
        "        if epoch > 0 and frozen:\n",
        "            for child in phoBERT_cls.children():\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "            frozen = False\n",
        "            del scheduler0\n",
        "            torch.cuda.empty_cache()\n",
        "        # Train model on EPOCH\n",
        "        print('Epoch: ', epoch)\n",
        "        trainOnEpoch(train_loader=train_loader, model=phoBERT_cls, optimizer=optimizer, epoch=epoch, num_epochs=EPOCHS, criteria=criteria, device=DEVICE, log_aggr=100)\n",
        "        # scheduler.step(epoch = epoch)\n",
        "        # Phá băng layers sau epoch đầu tiên\n",
        "        if not frozen:\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            scheduler0.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Validate on validation set\n",
        "        acc, f1 = validate(valid_loader, phoBERT_cls, device=DEVICE)\n",
        "        print('Epoch {} validation: acc: {:.4f}, f1: {:.4f} \\n'.format(epoch, acc, f1))\n",
        "\n",
        "        # Store best model checkpoint\n",
        "        ckpt_dict = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phoBERT_cls.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        # Save model checkpoint into 'latest_checkpoint.pth.tar'\n",
        "        torch.save(ckpt_dict, MODEL_LAST_CKPT)"
      ],
      "metadata": {
        "id": "pDd9tqXTtLsp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}