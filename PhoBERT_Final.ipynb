{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1q5Hjdzdj0NONfZ4DyK93tjrQL4lRugtP",
      "authorship_tag": "ABX9TyOLjOpAirj2xpl8XCAS2Sfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mynhungg/Datamining/blob/nguyen-anh-kiet/PhoBERT_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download package"
      ],
      "metadata": {
        "id": "iLG1gJaBhNzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL3nWAvcfSDp"
      },
      "outputs": [],
      "source": [
        "!pip3 install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install fastbpe"
      ],
      "metadata": {
        "id": "lzQ8ZalBhhSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install vncorenlp"
      ],
      "metadata": {
        "id": "UYtusxJJhpKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "7YSj3YWphtvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install sentencepiece"
      ],
      "metadata": {
        "id": "YfAmcD12hxya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords-dash.txt"
      ],
      "metadata": {
        "id": "sPjLYwT8h15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
        "!tar -xzvf PhoBERT_base_fairseq.tar.gz"
      ],
      "metadata": {
        "id": "Lyl1eQwMiKuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p vncorenlp/models/wordsegmenter"
      ],
      "metadata": {
        "id": "n_7xP2dMqGMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ "
      ],
      "metadata": {
        "id": "cDzTk_xgqgCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/"
      ],
      "metadata": {
        "id": "zh9iuVryqRZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "metadata": {
        "id": "e__glElIqRnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/duyvuleo/VNTC.git\n",
        "!ls VNTC/Data/10Topics/Ver1.1"
      ],
      "metadata": {
        "id": "fKQ9zkdBrww5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x VNTC/Data/10Topics/Ver1.1/Train_Full.rar\n",
        "!unrar x VNTC/Data/10Topics/Ver1.1/Test_Full.rar"
      ],
      "metadata": {
        "id": "enwlfH2i0dGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "K27iPStF0mRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "b7lZXtBm0rC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from vncorenlp import VnCoreNLP\n",
        "from fairseq.data import Dictionary\n",
        "from fairseq.data.encoders.fastbpe import fastBPE"
      ],
      "metadata": {
        "id": "GDeibDJz0pmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vietnamese word segmentation"
      ],
      "metadata": {
        "id": "bp9mm3Ev1FPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size=\"-Xmx500m\")"
      ],
      "metadata": {
        "id": "oDT4ZBcc128n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path difinition"
      ],
      "metadata": {
        "id": "xF1wKRFy15wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'Train_Full/*/*.txt'\n",
        "test_path = 'Test_Full/*/*.txt'"
      ],
      "metadata": {
        "id": "RwQmJy7h2cjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function difinition"
      ],
      "metadata": {
        "id": "OqI83cw028vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_txt(path):\n",
        "  with open(path, 'r', encoding='utf-16') as f:\n",
        "    data = f.read()\n",
        "  return data"
      ],
      "metadata": {
        "id": "XjLCiXQq3N7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stopwords_list():\n",
        "  with open('vietnamese-stopwords-dash.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "    stop_set = set(m.strip() for m in lines)\n",
        "    return list(frozenset(stop_set))"
      ],
      "metadata": {
        "id": "fu2UI1hu3VNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = get_stopwords_list()\n",
        "cnt = 0\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "  global cnt\n",
        "  tmp = texts.split(' ')\n",
        "  for sw in stopwords:\n",
        "    if sw in tmp:\n",
        "      cnt += 1\n",
        "      tmp.remove(sw)\n",
        "  return ' '.join(tmp)"
      ],
      "metadata": {
        "id": "3VPRkbI13dyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords2(words_arr):\n",
        "  global cnt\n",
        "  for words in words_arr:\n",
        "    for sw in stopwords:\n",
        "      if sw in words:\n",
        "        cnt += 1\n",
        "        words.remove(sw)\n",
        "  return words_arr"
      ],
      "metadata": {
        "id": "MaXysZM33iNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data(path):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  for file_path in tqdm(glob2.glob(path)):\n",
        "    try:\n",
        "      content = read_txt(file_path)\n",
        "      label = file_path.split('/')[9]\n",
        "      texts.append(content)\n",
        "      labels.append(label)\n",
        "    except:\n",
        "      next\n",
        "  return texts, labels"
      ],
      "metadata": {
        "id": "BSOlsTGM4DzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load function"
      ],
      "metadata": {
        "id": "GpawG3bG6HM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "ClNAzvCI6i72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _save_pkl(path, obj):\n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump(obj, f)"
      ],
      "metadata": {
        "id": "zGyAaveM6wh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_pkl(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    obj = pickle.load(f)\n",
        "  return obj"
      ],
      "metadata": {
        "id": "1k3At5Tn6x1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split test and label"
      ],
      "metadata": {
        "id": "ZMeo5aT1Pi0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_train, label_train = make_data(train_path)\n",
        "text_test, label_test = make_data(test_path)"
      ],
      "metadata": {
        "id": "OsNUr-JAQUrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_save_pkl('text_train.pkl', text_train)\n",
        "_save_pkl('label_train.pkl', label_train)\n",
        "_save_pkl('text_test.pkl', text_test)\n",
        "_save_pkl('label_test.pkl', label_test)"
      ],
      "metadata": {
        "id": "crIi3jhFQs4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize content"
      ],
      "metadata": {
        "id": "yungF0Z6RYta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "2jx5bXBwUgaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 256\n",
        "\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  outputs = np.zeros((len(lines), max_seq_len), dtype=np.int32)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for i, row in tqdm(enumerate(lines), total=len(lines)):\n",
        "    subwords = bpe.encode('<s> ' + row + ' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    if len(input_ids) > max_seq_len:\n",
        "      input_ids = input_ids[:max_seq_len]\n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      input_ids += [pad_id,]*(max_seq_len - len(input_ids))\n",
        "    \n",
        "    outputs[i, :] = np.array(input_ids)\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "QMLgy2VfRdal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dictionary"
      ],
      "metadata": {
        "id": "36XDbKas5UJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"PhoBERT_base_fairseq/dict.txt\")\n",
        "\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "args = BPE()\n",
        "bpe = fastBPE(args)"
      ],
      "metadata": {
        "id": "XrleHc595ZZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "Kl7Kz7L_U2JA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "v_PczdGjWC5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "pXj-lHG1WJpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "from transformers import *\n",
        "from transformers.modeling_utils import * \n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit"
      ],
      "metadata": {
        "id": "PfZZTU02WL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Argument definition"
      ],
      "metadata": {
        "id": "3tV6Q3XlWilS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "ACCUMULATION_STEPS = 5\n",
        "FOLD = 4\n",
        "LR = 2e-5\n",
        "LR_DC_STEP = 80\n",
        "LR_DC = 0.1\n",
        "CUR_DIR = os.path.dirname(os.getcwd())\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CKPT_PATH = 'model_ckpt_full_both'\n",
        "SEED = 69"
      ],
      "metadata": {
        "id": "d_7gmn2UWm2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing"
      ],
      "metadata": {
        "id": "17Jr4KnAW_xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = convert_lines(text_train, vocab, bpe)"
      ],
      "metadata": {
        "id": "5YAnqt31U42F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "lb.fit(label_train)\n",
        "y = lb.fit_transform(label_train)"
      ],
      "metadata": {
        "id": "kcJ0BKL8VFRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_save_pkl('/content/drive/MyDrive/Colab Notebooks/pkl/X_train_both.pkl', X)\n",
        "_save_pkl('/content/drive/MyDrive/Colab Notebooks/pkl/y_train_both.pkl', y)"
      ],
      "metadata": {
        "id": "p65o2pc6WzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(SEED):\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "zfwebhibXBoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(SEED)"
      ],
      "metadata": {
        "id": "rdFSpzLqXDWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(CKPT_PATH):\n",
        "  os.mkdir(CKPT_PATH)"
      ],
      "metadata": {
        "id": "kyD024cJXD3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = _load_pkl('X_train_both.pkl')\n",
        "y = _load_pkl('y_train_both.pkl')"
      ],
      "metadata": {
        "id": "toXE0ORUXJdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model pretrain PhoBERT"
      ],
      "metadata": {
        "id": "q0Ed7o3r5jIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.models.roberta import RobertaModel\n",
        "\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint='model.pt')\n",
        "phoBERT.bpe = bpe\n",
        "phoBERT.eval()"
      ],
      "metadata": {
        "id": "QZI328Aa5k1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phoBERT.register_classification_head('new_task', num_classes=10)\n",
        "phoBERT.to(DEVICE)"
      ],
      "metadata": {
        "id": "y9o68tuaZHpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating optimizer and learning rate schedulers"
      ],
      "metadata": {
        "id": "SbRJtgaLaigc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "  {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "  {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "num_train_optimization_steps = int(EPOCHS*len(X_train)/BATCH_SIZE/ACCUMULATION_STEPS)\n",
        "\n",
        "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LR)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)\n",
        "\n",
        "scheduler0 = get_constant_schedule(optimizer)\n",
        "\n",
        "criteria = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "FXbrS7wDa6RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function definition"
      ],
      "metadata": {
        "id": "0rBf-WB-bZGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(logits, targets):\n",
        "  logits = logits.detach().cpu().numpy()   \n",
        "\n",
        "  y_pred = np.argmax(logits, axis = 1)\n",
        "  targets = targets.detach().cpu().numpy()\n",
        "  \n",
        "  f1 = f1_score(targets, y_pred, average='weighted')\n",
        "  acc = accuracy_score(targets, y_pred)\n",
        "  return acc, f1"
      ],
      "metadata": {
        "id": "o7fE8YZdbcHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_loader, model, device):\n",
        "  model.eval()\n",
        "  accs = []\n",
        "  f1s = []\n",
        "  losses = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x_batch, y_batch in valid_loader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      outputs = model.predict('new_task', x_batch)\n",
        "\n",
        "      loss = criteria(outputs, y_batch)\n",
        "      logits = torch.exp(outputs)\n",
        "\n",
        "      acc, f1 = evaluate(logits, y_batch)\n",
        "      accs.append(acc)\n",
        "      \n",
        "      f1s.append(f1)\n",
        "      losses.append(loss.detach().cpu().numpy())\n",
        "  \n",
        "  mean_acc = np.mean(accs)\n",
        "  mean_f1 = np.mean(f1s)\n",
        "  mean_loss = np.mean(losses)\n",
        "  return mean_acc, mean_f1, mean_loss"
      ],
      "metadata": {
        "id": "m8lmJ0KrblVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_on_epoch(train_loader, model, optimizer, scheduler, epoch, num_epochs, criteria, device, log_aggr = 100):\n",
        "  model.train()\n",
        "  sum_epoch_loss = 0\n",
        "  sum_acc = 0\n",
        "  sum_f1 = 0\n",
        "  start = time.time()\n",
        "  train_size = len(train_loader)\n",
        "\n",
        "  for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    y_pred = model.predict('new_task', x_batch)\n",
        "    logits = torch.exp(y_pred)\n",
        "    acc, f1 = evaluate(logits, y_batch)\n",
        "    loss = criteria(y_pred, y_batch)\n",
        "\n",
        "    loss.backward()\n",
        "    if i % ACCUMULATION_STEPS == 0 or i == train_size- 1:\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      scheduler.step()\n",
        "\n",
        "    loss_val = loss.item()\n",
        "    sum_epoch_loss += loss_val\n",
        "    sum_acc += acc\n",
        "    sum_f1 += f1\n",
        "\n",
        "    if i % log_aggr == 0 or i == train_size - 1:\n",
        "      print(f'[TRAIN] epoch {epoch + 1}/{num_epochs}, \\\n",
        "        observation {i}/{train_size} batch, \\\n",
        "        loss: {loss:>.4f} (avg {(sum_epoch_loss / (i + 1)):>.4f}), \\\n",
        "        avg acc: {(sum_acc / (i + 1)):>.4f}, \\\n",
        "        avg f1: {(sum_f1 / (i + 1)):>.4f}, \\\n",
        "        ({(len(x_batch) / (time.time() - start)):>.2f} im/s)')\n",
        "    start = time.time()"
      ],
      "metadata": {
        "id": "ykCZ380tbsXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = list(StratifiedShuffleSplit(n_splits=1, random_state=123, test_size=0.1).split(X_train, y))"
      ],
      "metadata": {
        "id": "auvf7-rdcFWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(splits):\n",
        "  best_score = 0\n",
        "  print(\"Training for fold {}\".format(fold))\n",
        "\n",
        "  train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train[train_idx],dtype=torch.long), torch.tensor(y[train_idx],dtype=torch.long))\n",
        "  valid_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train[val_idx],dtype=torch.long), torch.tensor(y[val_idx],dtype=torch.long))\n",
        "  print('train size:', len(train_dataset))\n",
        "  print('valid size:', len(valid_dataset))\n",
        "\n",
        "  frozen = True\n",
        "\n",
        "  for epoch in tqdm(range(EPOCHS + 1)):\n",
        "    \n",
        "    if epoch > 0 and frozen:\n",
        "      for child in model.children():\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = True\n",
        "      frozen = False\n",
        "      del scheduler0\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print('\\nEpoch: ', epoch + 1)\n",
        "    optimizer.zero_grad()\n",
        "    train_on_epoch(train_loader=train_loader,\n",
        "                  model=model,\n",
        "                  optimizer=optimizer, \n",
        "                  epoch=epoch, \n",
        "                  num_epochs=EPOCHS + 1, \n",
        "                  criteria=criteria, \n",
        "                  device=DEVICE, \n",
        "                  scheduler=(scheduler0 if frozen else scheduler))\n",
        "    \n",
        "    acc, f1, loss = validate(valid_loader, model, device=DEVICE)\n",
        "    print('Epoch {} validation acc: {:.4f}, f1: {:.4f}, loss: {:.4f} \\n'.format(epoch + 1, acc, f1, loss))\n",
        "    if f1 >= best_score:\n",
        "      torch.save(model.state_dict(), os.path.join(CKPT_PATH, f'model.bin'))\n",
        "      best_score = f1"
      ],
      "metadata": {
        "id": "YdC6RZ-wcD4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "JG6NXGqccyZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = _load_pkl('PhoBERT_pretrain/X_test.pkl')\n",
        "y_test = _load_pkl('PhoBERT_pretrain/y_test.pkl')"
      ],
      "metadata": {
        "id": "vGAJc1lLc0mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "preds = []\n",
        "logits = []\n",
        "\n",
        "phoBERT.to(DEVICE)\n",
        "phoBERT.load_state_dict(torch.load(os.path.join('model_ckpt_full', 'model.bin')))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "phoBERT.eval()\n",
        "\n",
        "for i, (x_batch,) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "  logits = phoBERT.predict('new_task', x_batch)\n",
        "  probs = torch.exp(logits)\n",
        "  y_pred = np.argmax(probs.detach().cpu().numpy(), axis=1)\n",
        "  preds = np.concatenate([preds, y_pred])"
      ],
      "metadata": {
        "id": "eIbjJUhHdAdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "f1 = metrics.f1_score(y_test, preds, average=\"micro\")\n",
        "acc = metrics.accuracy_score(y_test, preds)\n",
        "print('f1 score: ', f1)\n",
        "print('accuracy: ', acc)"
      ],
      "metadata": {
        "id": "QOYYwn3CdFct"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}