{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mynhungg/Datamining/blob/Anh-Ki%E1%BB%87t/PhoBERT_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxC9LqUf8ocn",
        "outputId": "88d6b6fa-3828-4e28-d3df-91530f66af71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "JEHQ9q_R8px-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sLeeNguyen/nlp-text-classification.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orTF20EQ82tj",
        "outputId": "314fd1e2-be00-43dc-8b81-e8253d79a2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp-text-classification'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 17 (delta 4), reused 15 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), 162.50 KiB | 652.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nlp-text-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh1SyjM186hP",
        "outputId": "06c27a1d-0d44-4e56-caf1-13a69750225a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/nlp-text-classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x prepare.sh\n",
        "!./prepare.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTm7T0TcMNqQ",
        "outputId": "15962b9e-a484-4bcb-9b0c-12c5b73c3eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-26 06:59:54--  https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 13.35.166.28, 13.35.166.24, 13.35.166.17, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|13.35.166.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1243308020 (1.2G) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_fairseq.tar.gz’\n",
            "\n",
            "PhoBERT_base_fairse 100%[===================>]   1.16G  44.7MB/s    in 42s     \n",
            "\n",
            "2023-05-26 07:00:36 (28.4 MB/s) - ‘PhoBERT_base_fairseq.tar.gz’ saved [1243308020/1243308020]\n",
            "\n",
            "PhoBERT_base_fairseq/\n",
            "PhoBERT_base_fairseq/bpe.codes\n",
            "PhoBERT_base_fairseq/model.pt\n",
            "PhoBERT_base_fairseq/dict.txt\n",
            "--2023-05-26 07:01:20--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  45.5MB/s    in 0.6s    \n",
            "\n",
            "2023-05-26 07:01:21 (45.5 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2023-05-26 07:01:21--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-26 07:01:21 (9.24 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2023-05-26 07:01:21--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-26 07:01:22 (3.32 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lệnh **`chmod +x prepare.sh`** được sử dụng để cấp quyền thực thi cho tệp tin **`prepare.sh`**. Điều này cho phép chạy tệp tin shell **`prepare.sh`** như một chương trình thực thi.\n",
        "\n",
        "Lệnh **`./prepare.sh`** được sử dụng để thực thi tệp tin shell **`prepare.sh`**. Trong trường hợp này, khi chạy lệnh này, nó sẽ thực thi các câu lệnh được định nghĩa trong tệp tin **`prepare.sh`**."
      ],
      "metadata": {
        "id": "KiDgIU7jqX6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhrMlROO_uzY",
        "outputId": "7f6e2fbc-1a87-4a1a-98d0-1a923a262ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.32)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.64.1)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.6.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.4.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.6.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->fairseq) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->fairseq) (0.40.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vncorenlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqiFXpAk_wV0",
        "outputId": "5fd4a0fb-5933-402b-e233-6ee44b2cfab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vncorenlp in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA24xCun9kil",
        "outputId": "6e86c4a1-0654-447b-dd39-5e1f2a81d1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.8)\n",
            "Requirement already satisfied: anyio==3.6.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.6.2)\n",
            "Requirement already satisfied: autopep8==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: bitarray==2.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.6.0)\n",
            "Requirement already satisfied: certifi==2022.9.24 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2022.9.24)\n",
            "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.15.1)\n",
            "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.1.1)\n",
            "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (8.1.3)\n",
            "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.4.6)\n",
            "Requirement already satisfied: Cython==0.29.32 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.29.32)\n",
            "Requirement already satisfied: fairseq==0.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.12.2)\n",
            "Requirement already satisfied: fastapi==0.87.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.87.0)\n",
            "Requirement already satisfied: fastBPE==0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.1.0)\n",
            "Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.14.0)\n",
            "Requirement already satisfied: httptools==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.5.0)\n",
            "Requirement already satisfied: hydra-core==1.0.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.0.7)\n",
            "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.4)\n",
            "Requirement already satisfied: importlib-resources==5.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (5.10.0)\n",
            "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (1.2.0)\n",
            "Requirement already satisfied: lxml==4.9.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (4.9.1)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.23.5)\n",
            "Requirement already satisfied: omegaconf==2.0.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (2.0.6)\n",
            "Requirement already satisfied: portalocker==2.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (2.6.0)\n",
            "Requirement already satisfied: pycodestyle==2.9.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (2.9.1)\n",
            "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (2.21)\n",
            "Requirement already satisfied: pydantic==1.10.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (1.10.2)\n",
            "Requirement already satisfied: python-dotenv==0.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (0.21.0)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (6.0)\n",
            "Requirement already satisfied: regex==2022.10.31 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (2022.10.31)\n",
            "Requirement already satisfied: requests==2.28.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (2.28.1)\n",
            "Requirement already satisfied: sacrebleu==2.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (2.3.1)\n",
            "Requirement already satisfied: scikit-learn==1.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (1.1.3)\n",
            "Requirement already satisfied: scipy==1.9.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (1.9.3)\n",
            "Requirement already satisfied: sklearn==0.0.post1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (0.0.post1)\n",
            "Requirement already satisfied: sniffio==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (1.3.0)\n",
            "Requirement already satisfied: starlette==0.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (0.21.0)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (0.9.0)\n",
            "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (3.1.0)\n",
            "Requirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 39)) (2.0.1)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 40)) (1.13.0)\n",
            "Requirement already satisfied: torchaudio==0.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 41)) (0.13.0)\n",
            "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (4.64.1)\n",
            "Requirement already satisfied: typing_extensions==4.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (4.4.0)\n",
            "Requirement already satisfied: urllib3==1.26.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (1.26.12)\n",
            "Requirement already satisfied: uvicorn==0.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (0.20.0)\n",
            "Requirement already satisfied: uvloop==0.17.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (0.17.0)\n",
            "Requirement already satisfied: vncorenlp==1.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 47)) (1.0.3)\n",
            "Requirement already satisfied: watchfiles==0.18.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (0.18.1)\n",
            "Requirement already satisfied: websockets==10.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (10.4)\n",
            "Requirement already satisfied: zipp==3.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 50)) (3.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->-r requirements.txt (line 40)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->-r requirements.txt (line 40)) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->-r requirements.txt (line 40)) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->-r requirements.txt (line 40)) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->-r requirements.txt (line 40)) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->-r requirements.txt (line 40)) (0.40.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lệnh **`!pip install -r requirements.txt`** được sử dụng để cài đặt các gói phụ thuộc từ tệp tin **`requirements.txt`**."
      ],
      "metadata": {
        "id": "5F4lqWQArOIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from fairseq.models.roberta import RobertaModel, RobertaHubInterface\n",
        "from fairseq.data import Dictionary\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "\n",
        "from vncorenlp import VnCoreNLP"
      ],
      "metadata": {
        "id": "fFaZPpXTcZTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`import torch`**: Đây là thư viện được sử dụng cho việc tính toán số học và xây dựng mạng nơ-ron trong PyTorch.\n",
        "- **`import numpy as np`**: Đây là thư viện cho tính toán số học và xử lý mảng nhiều chiều trong Python.\n",
        "- **`import pickle`**: Đây là thư viện cho việc lưu trữ và truy xuất dữ liệu dưới dạng đối tượng Python bằng cách sử dụng giao thức pickle.\n",
        "- **`from sklearn.preprocessing import LabelEncoder`**: Đây là một lớp trong thư viện scikit-learn được sử dụng để mã hóa nhãn văn bản thành dạng số.\n",
        "-** `from fairseq.models.roberta import RobertaModel, RobertaHubInterface`**: Đây là các lớp và giao diện được cung cấp bởi fairseq, một thư viện sử dụng trong nghiên cứu và triển khai mô hình ngôn ngữ.\n",
        "- **`from fairseq.data import Dictionary`**: Đây là một lớp từ thư viện fairseq để xử lý từ điển văn bản.\n",
        "- **`from fairseq.data.encoders.fastbpe import fastBPE`**: Đây là một lớp từ fairseq để mã hóa văn bản bằng phương pháp mã hóa nhanh BPE (Byte Pair Encoding).\n",
        "- **`from vncorenlp import VnCoreNLP`**: Đây là một gói thư viện NLP cho tiếng Việt được gọi là VnCoreNLP, được sử dụng để xử lý văn bản tiếng Việt."
      ],
      "metadata": {
        "id": "lvxyJTPsr5N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "r2mpD7XscyBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.device('cuda' if torch.cuda.is_available() else 'cpu')` tạo một đối tượng thiết bị (device) trong PyTorch, với giá trị là `'cuda'` nếu có sẵn GPU hỗ trợ CUDA, ngược lại là `'cpu'`."
      ],
      "metadata": {
        "id": "KkRZdUrysdX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier():\n",
        "    def __init__(self) -> None:\n",
        "        self.max_seq_len = 256\n",
        "        self.__load_model()\n",
        "        self.__load_vocab()\n",
        "        self.__load_label_encoder()\n",
        "        self.__rdrsegmenter = VnCoreNLP(\n",
        "            \"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size=\"-Xmx500m\")\n",
        "\n",
        "    def get_model(self) -> RobertaModel:\n",
        "        return self.__model\n",
        "\n",
        "    def __load_model(self):\n",
        "        print('Loading NLP model...')\n",
        "        self.__model = RobertaModel.from_pretrained(\n",
        "            'PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "        self.__model.to(DEVICE)\n",
        "        self.__model.register_classification_head('new_task', num_classes=10)\n",
        "        self.__model.load_state_dict(torch.load(\n",
        "          '/content/drive/MyDrive/Colab Notebooks/VNTC/model_ckpt_full_both/model.bin', \n",
        "          map_location=DEVICE))\n",
        "\n",
        "        class BPE():\n",
        "            bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "        self.__model.bpe = self.__bpe = fastBPE(BPE())\n",
        "        print('\\nModel has loaded successfully.')\n",
        "\n",
        "    def __load_vocab(self):\n",
        "        print('\\nLoading dictionary...')\n",
        "        self.__vocab = Dictionary()\n",
        "        self.__vocab.add_from_file('PhoBERT_base_fairseq/dict.txt')\n",
        "        print('\\nDictionary has loaded')\n",
        "\n",
        "    def __load_label_encoder(self):\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/VNTC/labelEncoder.pkl', 'rb') as f:\n",
        "            self.le: LabelEncoder = pickle.load(f)\n",
        "\n",
        "    def encode(self, text) -> torch.LongTensor:\n",
        "        text_tokenized = ' '.join([\n",
        "            ' '.join(sent) for sent in self.__rdrsegmenter.tokenize(text)\n",
        "        ])\n",
        "        bpe_sentence = '<s> ' + self.__bpe.encode(text_tokenized) + ' <s>'\n",
        "        tokens = self.__vocab.encode_line(\n",
        "            bpe_sentence, append_eos=False, add_if_not_exist=False)\n",
        "        # Index cac token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "        # cls_id = 0\n",
        "        eos_id = 2\n",
        "        pad_id = 1\n",
        "        if len(tokens) > self.max_seq_len:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "            tokens[-1] = eos_id\n",
        "        else:\n",
        "            tokens = torch.cat((tokens, torch.tensor(\n",
        "                [pad_id, ] * (self.max_seq_len - len(tokens)))))\n",
        "        return tokens.long()\n",
        "\n",
        "    def predict(self, text):\n",
        "        self.__model.eval()\n",
        "        tokens = self.encode(text)\n",
        "        logits = self.__model.predict('new_task', tokens, return_logits=True)\n",
        "        y_pred = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "        return self.le.inverse_transform(y_pred)[0]"
      ],
      "metadata": {
        "id": "czt9yPGLcT5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lớp **`TextClassifier`** là một lớp đóng gói các chức năng liên quan đến phân loại văn bản. Dưới đây là giải thích về từng phương thức và thuộc tính trong lớp này:\n",
        "\n",
        "- **`__init__(self)`**: Phương thức khởi tạo của lớp. Nó thiết lập giá trị cho các thuộc tính **`max_seq_len`**, và gọi các phương thức **`__load_model()`**, **`__load_vocab()`**, **`__load_label_encoder()`** để tải các thành phần cần thiết.\n",
        "\n",
        "- **`get_model(self) -> RobertaModel`**: Phương thức trả về mô hình **`RobertaModel`** được sử dụng bên trong lớp.\n",
        "\n",
        "- **`__load_model(self)`**: Phương thức tải mô hình **`RobertaModel`** từ tệp checkpoint đã được huấn luyện trước và tải trọng số lưu trữ trong tệp **`model.bin**`. Nó cũng cấu hình mô hình cho tác vụ phân loại mới với số lớp là 10.\n",
        "\n",
        "- **`__load_vocab(self)`**: Phương thức tải từ điển từ vựng từ tệp `dict.txt`.\n",
        "\n",
        "- **`__load_label_encoder(self)`**: Phương thức tải trình mã hóa nhãn từ tệp **`labelEncoder.pkl`** bằng cách sử dụng **`pickle`**.\n",
        "\n",
        "- **`encode(self, text) -> torch.LongTensor`**: Phương thức mã hóa một đoạn văn bản thành một tensor **`torch.LongTensor`** sử dụng từ điển từ vựng. Nó chia văn bản thành các câu và mã hóa chúng bằng phương pháp mã hóa BPE. Đoạn văn bản được mã hóa sẽ có chiều dài tối đa là **`max_seq_len`**.\n",
        "\n",
        "- **`predict(self, text)`**: Phương thức dự đoán nhãn cho một đoạn văn bản. Nó mã hóa đoạn văn bản và sử dụng mô hình để dự đoán nhãn. Cuối cùng, nó chuyển đổi nhãn dự đoán từ dạng số sang dạng chuỗi bằng cách sử dụng **`LabelEncoder`**."
      ],
      "metadata": {
        "id": "7A9cRSW9s98k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = TextClassifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuxwSvL7dlvk",
        "outputId": "ea2d5f68-a12d-4a67-8697-2f4e17039f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading NLP model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1042301B [00:00, 52656821.04B/s]\n",
            "456318B [00:00, 7218206.83B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model has loaded successfully.\n",
            "\n",
            "Loading dictionary...\n",
            "\n",
            "Dictionary has loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "       Sau này có rất nhiều lần trời đổ mưa, nhưng anh đã không đội mưa đến gặp em thêm một lần nào nữa.\n",
        "       \n",
        "       '''"
      ],
      "metadata": {
        "id": "MN9kbzi5cm0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier.predict(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv4B9N56ijNY",
        "outputId": "56344962-0ca2-459a-99e5-564073b9a017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doi song\n"
          ]
        }
      ]
    }
  ]
}