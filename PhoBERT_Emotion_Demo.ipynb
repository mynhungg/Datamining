{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "jP47odF8QO6P",
        "gwq19_GDhjfj",
        "14a8PzgkU3ZL"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mynhungg/Datamining/blob/main/PhoBERT_Emotion_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Prepare"
      ],
      "metadata": {
        "id": "hGWb3HFky5X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive"
      ],
      "metadata": {
        "id": "jP47odF8QO6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC0fLCfsvr-g",
        "outputId": "5a8e6c5c-5abd-4c28-fd3b-f33f3814cd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/PhoBERT_Emotion\"\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "EnyVC7XJeSVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system('ls')"
      ],
      "metadata": {
        "id": "o11RD_SRebju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000475b6-8fab-4343-be8d-055c095b60a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " phobert_fold1.pth   phobert_fold4.pth\t\t\t UIT-VSMEC\n",
            " phobert_fold2.pth   phobert_fold5.pth\n",
            " phobert_fold3.pth  'Screenshot 2023-06-01 000433.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lệnh **`get_ipython().system('ls')`** được sử dụng trong môi trường Jupyter Notebook hoặc IPython để thực thi lệnh hệ thống **`ls`** (liệt kê các tệp tin và thư mục) và hiển thị kết quả trong ô output của Notebook."
      ],
      "metadata": {
        "id": "xnEdrg4Dedj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download packages"
      ],
      "metadata": {
        "id": "gwq19_GDhjfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppQiKV1_yz_J",
        "outputId": "ebda1b92-8ffe-45a6-b16e-5d68f32d1ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`openpyxl`** là một thư viện Python được sử dụng để làm việc với các tệp Excel (**`.xlsx`**). Nó cung cấp các công cụ cho việc đọc, ghi và chỉnh sửa dữ liệu trong các tệp Excel, cho phép thao tác với các bảng tính, các ô và các công thức trong tệp Excel."
      ],
      "metadata": {
        "id": "57lwf2BAgFxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oipm1og9101u",
        "outputId": "2012631a-e457-4b1c-e066-092654fc7f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gói **`transformers`** là một thư viện Python phổ biến được phát triển bởi Hugging Face. Nó cung cấp các công cụ và mô hình để làm việc với xử lý ngôn ngữ tự nhiên (**`NLP`**) và học sâu (**`deep learning`**). Thư viện này cung cấp các mô hình nổi tiếng như **`BERT, GPT, RoBERTa`** và nhiều mô hình khác, và cung cấp các chức năng để tải, huấn luyện và sử dụng những mô hình này trong các tác vụ NLP."
      ],
      "metadata": {
        "id": "UFfILWWOgVOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "D062BinahnZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, logging\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "UeC0HqkNyzaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`torch`**: Đây là thư viện PyTorch, một thư viện mã nguồn mở cho việc tính toán số học trên GPU và xây dựng các mô hình học sâu.\n",
        "- **`numpy as np`**: Thư viện NumPy, được sử dụng để làm việc với mảng và ma trận số học.\n",
        "- **`pandas as pd`**: Thư viện Pandas, được sử dụng để làm việc với dữ liệu dạng bảng và thực hiện các thao tác xử lý dữ liệu.\n",
        "- **`seaborn as sns`**: Thư viện Seaborn, được sử dụng để tạo biểu đồ và trực quan hóa dữ liệu.\n",
        "- **`matplotlib.pyplot as plt`**: Một phần của thư viện Matplotlib, được sử dụng để tạo biểu đồ và trực quan hóa dữ liệu.\n",
        "- **`gensim.utils.simple_preprocess`**: Một phần của thư viện Gensim, được sử dụng để tiền xử lý văn bản và chuyển đổi văn bản thành các từ đơn giản.\n",
        "- **`sklearn.model_selection.StratifiedKFold`**: Một phần của thư viện Scikit-learn, được sử dụng để thực hiện phân chia dữ liệu theo Stratified K-Fold cho việc cross-validation.\n",
        "- **`sklearn.metrics.classification_report`**: Một phần của thư viện Scikit-learn, được sử dụng để tính toán báo cáo phân loại dựa trên các chỉ số như độ chính xác, độ phủ và f1-score.\n",
        "- **`sklearn.metrics.confusion_matrix`**: Một phần của thư viện Scikit-learn, được sử dụng để tính toán ma trận nhầm lẫn (confusion matrix) cho các bài toán phân loại.\n",
        "- **`torch.nn`**: Mô-đun trong thư viện PyTorch cho các lớp mô hình và các hàm mất mát (loss functions).\n",
        "- **`torch.optim.AdamW`**: Một phần của thư viện PyTorch, cung cấp trình tối ưu hóa AdamW.\n",
        "- **`torch.utils.data.Dataset, torch.utils.data.DataLoader`**: Các phần của thư viện PyTorch dùng để xây dựng và tải dữ liệu vào mô hình.\n",
        "- **`transformers`**: Thư viện Hugging Face Transformers, được sử dụng để làm việc với các mô hình NLP đã được huấn luyện trước (pretrained models) như BERT, GPT, RoBERTa, và các hàm tiện ích liên quan.\n",
        "- **`warnings`**: Một mô-đun trong Python để quản lý cảnh báo và thông báo trong quá trình chạy mã."
      ],
      "metadata": {
        "id": "J1fxAwqVhCE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function definition"
      ],
      "metadata": {
        "id": "uxQRaxDXhsfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed_value):\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "C_0BQnyNy2pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đoạn mã trên định nghĩa một hàm **`seed_everything`** để thiết lập các giá trị hạt giống (seed) cho việc tái lặp lại (reproducibility) quá trình huấn luyện mô hình. Dưới đây là ý nghĩa của từng dòng mã trong hàm:\n",
        "\n",
        "- **`np.random.seed(seed_value)`**: Thiết lập giá trị hạt giống cho thư viện NumPy để tái lặp lại các phép ngẫu nhiên.\n",
        "- **`torch.manual_seed(seed_value)`**: Thiết lập giá trị hạt giống cho thư viện PyTorch để tái lặp lại các phép ngẫu nhiên.\n",
        "- **`if torch.cuda.is_available()`**: Kiểm tra xem có sử dụng GPU (CUDA) hay không.\n",
        "- **`torch.cuda.manual_seed(seed_value)`**: Thiết lập giá trị hạt giống cho CUDA để tái lặp lại các phép ngẫu nhiên trên GPU.\n",
        "- **`torch.cuda.manual_seed_all(seed_value)`**: Thiết lập giá trị hạt giống cho tất cả các thiết bị CUDA (nếu có).\n",
        "- **`torch.backends.cudnn.deterministic = True`**: Đặt chế độ xác định (deterministic) cho việc sử dụng thư viện cuDNN trên GPU. Điều này đảm bảo kết quả của các phép tính trên GPU là nhất quán và tái lặp lại được.\n",
        "- **`torch.backends.cudnn.benchmark = True`**: Đặt chế độ benchmark cho việc sử dụng thư viện cuDNN trên GPU. Điều này cho phép PyTorch tự động tinh chỉnh cấu hình cuDNN để tối ưu hóa hiệu suất.\n",
        "\n",
        "Bằng cách thiết lập các giá trị hạt giống như trên, ta có thể đảm bảo rằng quá trình huấn luyện mô hình sẽ cho ra kết quả nhất quán và có thể tái lặp lại được khi chạy nhiều lần."
      ],
      "metadata": {
        "id": "fR6Jm-jgiclX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(86)"
      ],
      "metadata": {
        "id": "gYwlDnqQhaSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(path):\n",
        "    df = pd.read_excel(path, sheet_name='Sheet1')\n",
        "    df.columns = ['index', 'Emotion', 'Sentence']\n",
        "    df.drop(columns=['index'], inplace=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "tE-LWb0lkaZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm **`get_data`** trong đoạn mã trên nhận đầu vào là đường dẫn tới tệp Excel và trả về một dataframe chứa dữ liệu.\n",
        "\n",
        "Các bước trong hàm **`get_data`** là như sau:\n",
        "\n",
        "1. **`df = pd.read_excel(path, sheet_name='Sheet1')`**: Đọc tệp Excel từ đường dẫn **`path`** và lưu dữ liệu vào dataframe **`df`**. Tham số **`sheet_name='Sheet1'`** chỉ định rằng dữ liệu được đọc từ sheet có tên là 'Sheet1'.\n",
        "2. **`df.columns = ['index', 'Emotion', 'Sentence']`**: Đặt tên cột cho dataframe **`df`** thành ['index', 'Emotion', 'Sentence']. Các cột này tương ứng với các cột trong tệp Excel.\n",
        "3. **`df.drop(columns=['index'], inplace=True)`**: Xóa cột 'index' khỏi dataframe **`df`** vì không cần thiết.\n",
        "4. **`return df`**: Trả về dataframe **`df`** đã xử lý."
      ],
      "metadata": {
        "id": "5MRpftsakrix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, train_loader):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_masks'].to(device)\n",
        "        targets = data['targets'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct += torch.sum(pred == targets)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(f'Train Accuracy: {correct.double()/len(train_loader.dataset)} Loss: {np.mean(losses)}')"
      ],
      "metadata": {
        "id": "5VNh49eNqKG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm **`train`** được sử dụng để huấn luyện mô hình. Các tham số đầu vào bao gồm:\n",
        "\n",
        "- **`model`**: Mô hình **`SentimentClassifier`** đã được khởi tạo.\n",
        "- **`criterion`**: Hàm mất mát để tính toán độ lỗi (ví dụ: **`nn.CrossEntropyLoss`**).\n",
        "- **`optimizer`**: Bộ tối ưu hóa được sử dụng để cập nhật trọng số của mô hình (ví dụ: **`AdamW`**).\n",
        "- **`train_loader`**: **`DataLoader`** chứa dữ liệu huấn luyện.\n",
        "\n",
        "Trong quá trình huấn luyện, các bước sau được thực hiện:\n",
        "\n",
        "- Đặt mô hình vào chế độ huấn luyện bằng cách gọi **`model.train()`**.\n",
        "- Khởi tạo biến **`losses`** để lưu trữ giá trị mất mát từ các batch.\n",
        "- Khởi tạo biến **`correct`** để lưu trữ số lượng dự đoán chính xác.\n",
        "- Với mỗi batch trong **`train_loader`**, thực hiện các bước sau:\n",
        "  - Di chuyển dữ liệu vào GPU (nếu có) bằng cách gọi **`to(device)`**.\n",
        "  - Xóa các gradient trước khi tính toán backward bằng **`optimizer.zero_grad()`**.\n",
        "  - Đưa dữ liệu qua mô hình để có đầu ra (**`outputs`**).\n",
        "  - Tính toán giá trị mất mát bằng cách so sánh đầu ra và nhãn thật (**`targets`**) thông qua **`criterion`**.\n",
        "  - Dùng **`torch.max`** để lấy ra nhãn dự đoán có giá trị lớn nhất.\n",
        "  - Cập nhật biến **`correct`** bằng cách tính toán số lượng dự đoán chính xác.\n",
        "  - Lưu trữ giá trị mất mát vào biến **`losses`**.\n",
        "  - Tính gradient và cập nhật trọng số mô hình thông qua **`loss.backward()`** và **`optimizer.step()`**.\n",
        "  - Áp dụng quá trình điều chỉnh learning rate thông qua **`lr_scheduler.step()`** (nếu có).\n",
        "  \n",
        "Cuối cùng, hàm in ra thông tin về độ chính xác trên tập huấn luyện và giá trị trung bình của mất mát (**`np.mean(losses)`**)."
      ],
      "metadata": {
        "id": "sjegJSWArrhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(test_data = False):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data_loader = test_loader if test_data else valid_loader\n",
        "        for data in data_loader:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_masks'].to(device)\n",
        "            targets = data['targets'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            correct += torch.sum(pred == targets)\n",
        "            losses.append(loss.item())\n",
        "    \n",
        "    if test_data:\n",
        "        print(f'Test Accuracy: {correct.double()/len(test_loader.dataset)} Loss: {np.mean(losses)}')\n",
        "        return correct.double()/len(test_loader.dataset)\n",
        "    else:\n",
        "        print(f'Valid Accuracy: {correct.double()/len(valid_loader.dataset)} Loss: {np.mean(losses)}')\n",
        "        return correct.double()/len(valid_loader.dataset)"
      ],
      "metadata": {
        "id": "cYCqXblk94Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm **`eval`** được sử dụng để đánh giá hiệu suất của mô hình trên tập kiểm tra hoặc tập xác thực. Các tham số đầu vào bao gồm:\n",
        "\n",
        "- **`test_data`**: Một cờ boolean xác định liệu ta đang đánh giá trên tập kiểm tra (**`True`**) hay tập xác thực (**`False`**).\n",
        "\n",
        "Trong quá trình đánh giá, các bước sau được thực hiện:\n",
        "\n",
        "- Đặt mô hình vào chế độ đánh giá bằng cách gọi **`model.eval()`**.\n",
        "- Khởi tạo biến **`losses`** để lưu trữ giá trị mất mát từ các batch.\n",
        "- Khởi tạo biến **`correct`** để lưu trữ số lượng dự đoán chính xác.\n",
        "- Sử dụng **`torch.no_grad()`** để tắt tính toán gradient.\n",
        "- Chọn **`data_loader`** tương ứng (từ **`test_loader`** hoặc **`valid_loader`**) dựa trên giá trị của **`test_data`**.\n",
        "- Với mỗi batch trong **`data_loader`**, thực hiện các bước sau:\n",
        "  - Di chuyển dữ liệu vào GPU (nếu có) bằng cách gọi **`to(device)`**.\n",
        "  - Đưa dữ liệu qua mô hình để có đầu ra (**`outputs`**).\n",
        "  - Dùng **`torch.max`** để lấy ra nhãn dự đoán có giá trị lớn nhất.\n",
        "  - Tính toán giá trị mất mát bằng cách so sánh đầu ra và nhãn thật thông qua **`criterion`**.\n",
        "  - Cập nhật biến **`correct`** bằng cách tính toán số lượng dự đoán chính xác.\n",
        "  - Lưu trữ giá trị mất mát vào biến **`losses`**.\n",
        "\n",
        "Cuối cùng, hàm in ra thông tin về độ chính xác trên tập kiểm tra hoặc tập xác thực và giá trị trung bình của mất mát (**`np.mean(losses)`**). Hàm trả về độ chính xác tính được trên tập kiểm tra hoặc tập xác thực."
      ],
      "metadata": {
        "id": "fWLV3gbttQHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_loaders(df, fold):\n",
        "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "    \n",
        "    train_dataset = SentimentDataset(df_train, tokenizer, max_len=120)\n",
        "    valid_dataset = SentimentDataset(df_valid, tokenizer, max_len=120)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "    \n",
        "    return train_loader, valid_loader"
      ],
      "metadata": {
        "id": "N-wGT1kZ97WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm **`prepare_loaders`** được sử dụng để chuẩn bị các DataLoader cho việc huấn luyện và đánh giá trên mỗi fold. Các tham số đầu vào bao gồm:\n",
        "\n",
        "- **`df`**: DataFrame chứa dữ liệu huấn luyện đã được gán fold.\n",
        "- **`fold`**: Fold hiện tại đang được xử lý.\n",
        "\n",
        "Các bước thực hiện trong hàm là:\n",
        "\n",
        "- Tách DataFrame **`df`** thành **`df_train`** và **`df_valid`** dựa trên fold hiện tại.\n",
        "- Tạo đối tượng **`train_dataset`** và **`valid_dataset`** từ **`df_train`** và **`df_valid`** sử dụng lớp **`SentimentDataset`**.\n",
        "- Tạo DataLoader **`train_loader`** và **`valid_loader`** từ **`train_dataset`** và **`valid_dataset`** với các thông số:\n",
        "  - **`batch_size=16`**: Số lượng mẫu trong mỗi batch.\n",
        "  - **`shuffle=True`**: Xáo trộn dữ liệu trước khi tạo batch.\n",
        "  - **`num_workers=2`**: Số luồng xử lý dữ liệu song song.\n",
        "\n",
        "Cuối cùng, hàm trả về **`train_loader`** và **`valid_loader`** để sử dụng trong quá trình huấn luyện và đánh giá."
      ],
      "metadata": {
        "id": "lp-Nqr8yunxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(data_loader):\n",
        "    models = []\n",
        "    for fold in range(skf.n_splits):\n",
        "        model = SentimentClassifier(n_classes=7)\n",
        "        model.to(device)\n",
        "        model.load_state_dict(torch.load(f'phobert_fold{fold+1}.pth', map_location=torch.device('cpu')))\n",
        "        model.eval()\n",
        "        models.append(model)\n",
        "\n",
        "    texts = []\n",
        "    predicts = []\n",
        "    predict_probs = []\n",
        "    real_values = []\n",
        "\n",
        "    for data in data_loader:\n",
        "        text = data['text']\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_masks'].to(device)\n",
        "        targets = data['targets'].to(device)\n",
        "\n",
        "        total_outs = []\n",
        "        for model in models:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                )\n",
        "                total_outs.append(outputs)\n",
        "        \n",
        "        total_outs = torch.stack(total_outs)\n",
        "        _, pred = torch.max(total_outs.mean(0), dim=1)\n",
        "        texts.extend(text)\n",
        "        predicts.extend(pred)\n",
        "        predict_probs.extend(total_outs.mean(0))\n",
        "        real_values.extend(targets)\n",
        "    \n",
        "    predicts = torch.stack(predicts).cpu()\n",
        "    predict_probs = torch.stack(predict_probs).cpu()\n",
        "    real_values = torch.stack(real_values).cpu()\n",
        "    print(classification_report(real_values, predicts))\n",
        "    return real_values, predicts"
      ],
      "metadata": {
        "id": "y8OE9Nhs-IHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm **`test`** được sử dụng để đánh giá mô hình trên dữ liệu kiểm tra hoặc dữ liệu mới. Các bước thực hiện trong hàm **`test`** là:\n",
        "\n",
        "- Khởi tạo một danh sách **`models`** để lưu trữ các mô hình đã được huấn luyện trên các fold trước đó.\n",
        "- Vòng lặp qua các fold để tải và tạo mô hình từ trạng thái đã lưu.\n",
        "- Thiết lập mô hình ở chế độ đánh giá bằng cách gọi **`model.eval()`**.\n",
        "- Tạo các danh sách để lưu trữ dữ liệu về văn bản, dự đoán, xác suất dự đoán và giá trị thực của các mẫu.\n",
        "- Vòng lặp qua từng mẫu trong **`data_loader`**.\n",
        "  - Lấy thông tin văn bản, đầu vào **`input_ids`**, **`attention_mask`** và giá trị thực **`targets`**.\n",
        "  - Tạo danh sách **`total_outs`** để lưu trữ đầu ra của các mô hình trên từng fold.\n",
        "  - Vòng lặp qua các mô hình trong **`models`**.\n",
        "    - Sử dụng mô hình để tính toán đầu ra cho mẫu đầu vào.\n",
        "    - Lưu trữ đầu ra vào danh sách **`total_outs`**.\n",
        "  - Chuyển đổi danh sách **`total_outs`** thành tensor và tính toán dự đoán cuối cùng bằng cách lấy giá trị lớn nhất trung bình.\n",
        "  - Gắn kết thông tin văn bản, dự đoán, xác suất dự đoán và giá trị thực vào các danh sách tương ứng.\n",
        "- Chuyển đổi dự đoán, xác suất dự đoán và giá trị thực thành tensor trên CPU.\n",
        "- In báo cáo phân loại bằng cách gọi **`classification_report`** từ **`sklearn.metrics`**.\n",
        "- Trả về các tensor **`real_values`** (giá trị thực) và **`predicts`** (dự đoán)."
      ],
      "metadata": {
        "id": "iFXG1aKQwmnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_wrong(real_values, predicts):\n",
        "    wrong_arr = []\n",
        "    wrong_label = []\n",
        "    for i in range(len(predicts)):\n",
        "        if predicts[i] != real_values[i]:\n",
        "            wrong_arr.append(i)\n",
        "            wrong_label.append(predicts[i])\n",
        "    return wrong_arr, wrong_label"
      ],
      "metadata": {
        "id": "RGdnRkcKz_5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm **`check_wrong`** được sử dụng để kiểm tra các trường hợp dự đoán sai của mô hình.\n",
        "\n",
        "- Đầu vào của hàm là **`real_values`** (giá trị thực) và **`predicts`** (dự đoán).\n",
        "- Hàm sẽ duyệt qua từng phần tử trong **`predicts`** và so sánh với giá trị tương ứng trong **`real_values`**.\n",
        "- Nếu dự đoán không khớp với giá trị thực, vị trí của phần tử đó sẽ được thêm vào danh sách **`wrong_arr`**, và dự đoán sai sẽ được thêm vào danh sách **`wrong_label`**.\n",
        "- Cuối cùng, hàm trả về **`wrong_arr`** (danh sách các vị trí dự đoán sai) và **`wrong_label`** (danh sách các dự đoán sai)."
      ],
      "metadata": {
        "id": "nx5V0lvn0PJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(text, tokenizer, max_len=120):\n",
        "    encoded_review = tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    input_ids = encoded_review['input_ids'].to(device)\n",
        "    attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "    output = model(input_ids, attention_mask)\n",
        "    _, y_pred = torch.max(output, dim=1)\n",
        "\n",
        "    print(f'Text: {text}')\n",
        "    print(f'Sentiment: {class_names[y_pred]}')"
      ],
      "metadata": {
        "id": "ENbHjI7w-aUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đoạn code trên định nghĩa một hàm **`infer`** để dự đoán cảm xúc của một câu văn mới bằng cách sử dụng mô hình đã được huấn luyện. \n",
        "\n",
        "Hàm **`infer`** nhận vào các đối số sau:\n",
        "- **`text`**: Đây là câu văn cần dự đoán cảm xúc.\n",
        "- **`tokenizer`**: Đối tượng tokenizer được sử dụng để mã hóa câu văn thành dạng phù hợp cho mô hình.\n",
        "- **`max_len`**: Độ dài tối đa của câu văn sau khi mã hóa.\n",
        "\n",
        "Trước tiên, câu văn **`text`** được mã hóa bằng cách sử dụng tokenizer. Quá trình mã hóa này bao gồm thêm các token đặc biệt vào câu văn, cắt ngắn hoặc bổ sung các token để đảm bảo độ dài của câu văn không vượt quá **`max_len`**, và tạo các tensor đại diện cho câu văn (bao gồm tensor **`input_ids`** và **`attention_mask`**).\n",
        "\n",
        "Tiếp theo, câu văn được đưa vào mô hình để dự đoán cảm xúc. Mô hình trả về một tensor chứa xác suất dự đoán cho từng lớp cảm xúc. Bằng cách sử dụng hàm **`torch.max`**, ta lấy chỉ số của lớp có xác suất dự đoán cao nhất (**`y_pred`**).\n",
        "\n",
        "Cuối cùng, hàm **`infer`** in ra câu văn ban đầu (**`text`**) và cảm xúc dự đoán tương ứng (**`class_names[y_pred]`**). **`class_names`** là danh sách các nhãn cảm xúc tương ứng với các lớp.\n",
        "\n",
        "Bằng cách sử dụng hàm **`infer`**, ta có thể dễ dàng dự đoán cảm xúc của các câu văn mới bằng cách gọi hàm và truyền vào câu văn cần dự đoán."
      ],
      "metadata": {
        "id": "8pmadUxe2FhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class definition"
      ],
      "metadata": {
        "id": "14a8PzgkU3ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=120):\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        To customize dataset, inherit from Dataset class and implement\n",
        "        __len__ & __getitem__\n",
        "        __getitem__ should return \n",
        "            data:\n",
        "                input_ids\n",
        "                attention_masks\n",
        "                text\n",
        "                targets\n",
        "        \"\"\"\n",
        "        row = self.df.iloc[index]\n",
        "        text, label = self.get_input_data(row)\n",
        "\n",
        "        # Encode_plus will:\n",
        "        # (1) split text into token\n",
        "        # (2) Add the '[CLS]' and '[SEP]' token to the start and end\n",
        "        # (3) Truncate/Pad sentence to max length\n",
        "        # (4) Map token to their IDS\n",
        "        # (5) Create attention mask\n",
        "        # (6) Return a dictionary of outputs\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_masks': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "    def labelencoder(self,text):\n",
        "        if text=='Enjoyment':\n",
        "            return 0\n",
        "        elif text=='Disgust':\n",
        "            return 1\n",
        "        elif text=='Sadness':\n",
        "            return 2\n",
        "        elif text=='Anger':\n",
        "            return 3\n",
        "        elif text=='Surprise':\n",
        "            return 4\n",
        "        elif text=='Fear':\n",
        "            return 5\n",
        "        else:\n",
        "            return 6\n",
        "\n",
        "    def get_input_data(self, row):\n",
        "        # Preprocessing: {remove icon, special character, lower}\n",
        "        text = row['Sentence']\n",
        "        text = ' '.join(simple_preprocess(text))\n",
        "        label = self.labelencoder(row['Emotion'])\n",
        "\n",
        "        return text, label"
      ],
      "metadata": {
        "id": "FX6tP3CU9neg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lớp **`SentimentDataset`** là một lớp con của **`torch.utils.data.Dataset`**, được sử dụng để tạo ra một tập dữ liệu tùy chỉnh cho nhiệm vụ phân loại cảm xúc (sentiment classification).\n",
        "\n",
        "Các phương thức chính trong lớp **`SentimentDataset`** bao gồm:\n",
        "\n",
        "- **`__init__(self, df, tokenizer, max_len=120)`**: Phương thức khởi tạo của lớp, nhận vào dataframe **`df`** chứa dữ liệu huấn luyện, tokenizer **`tokenizer`** và độ dài tối đa **`max_len`** của câu.\n",
        "- **`__len__(self)`**: Phương thức trả về số lượng mẫu trong tập dữ liệu.\n",
        "- **`__getitem__(self, index)`**: Phương thức trả về một mẫu dữ liệu tại vị trí **`index`**. Phương thức này thực hiện tiền xử lý văn bản, mã hóa văn bản thành các mã token và trả về một từ điển chứa thông tin của mẫu dữ liệu, bao gồm **`input_ids`** (mã token), **`attention_masks`** (mặt nạ chú ý), **`text`** (văn bản gốc) và **`targets`** (nhãn).\n",
        "- **`labelencoder(self, text)`**: Phương thức chuyển đổi nhãn từ dạng văn bản sang số nguyên. Các nhãn được mã hóa như sau: **`Enjoyment`**: 0, **`Disgust`**: 1, **`Sadness`**: 2, **`Anger`**: 3, **`Surprise`**: 4, **`Fear`**: 5, **`Khác`**: 6.\n",
        "- **`get_input_data(self, row)`**: Phương thức thực hiện xử lý dữ liệu đầu vào từ một hàng dữ liệu trong dataframe **`df`**. Nó loại bỏ các ký tự đặc biệt, chuyển đổi văn bản thành dạng lowercase và trả về văn bản và nhãn tương ứng.\n",
        "\n",
        "Lớp **`SentimentDataset`** cho phép tạo ra các đối tượng tập dữ liệu tuỳ chỉnh để huấn luyện và đánh giá mô hình phân loại cảm xúc."
      ],
      "metadata": {
        "id": "TyV9hp_KnyoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        last_hidden_state, output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=False # Dropout will errors if without this\n",
        "        )\n",
        "\n",
        "        x = self.drop(output)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5o_hv7_a9sXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lớp **`SentimentClassifier`** là một mô hình phân loại cảm xúc dựa trên kiến trúc của RoBERTa. Cụ thể:\n",
        "\n",
        "- **`n_classes`**: Số lượng lớp đầu ra (số lượng cảm xúc khác nhau).\n",
        "\n",
        "Phương thức **`__init__`**:\n",
        "- Khởi tạo mô hình RoBERTa (**`self.bert`**) từ pretrained weights của bộ mã hóa PhoBERT.\n",
        "- Áp dụng Dropout với xác suất 0.3 (**`self.drop`**) để tránh overfitting.\n",
        "- Tạo một lớp tuyến tính (**`self.fc`**) với kích thước đầu ra bằng **`n_classes`** và khởi tạo các trọng số ngẫu nhiên theo phân phối chuẩn.\n",
        "\n",
        "Phương thức **`forward`**:\n",
        "- Đầu vào của mô hình là **`input_ids`** (chuỗi các mã token) và **`attention_mask`** (mảng chỉ định phần tử thực tế và padding trong câu).\n",
        "- Mô hình RoBERTa được áp dụng cho đầu vào và trả về kết quả cuối cùng và hidden state (**`last_hidden_state`**, **`output`**).\n",
        "- Đầu ra từ RoBERTa được truyền qua Dropout layer và sau đó thông qua một lớp tuyến tính để đưa ra dự đoán cho từng lớp.\n",
        "\n",
        "Lớp **`SentimentClassifier`** là một lớp con của lớp **`nn.Module`** trong PyTorch và triển khai phương thức **`forward`** để thực hiện feedforward qua mô hình."
      ],
      "metadata": {
        "id": "yHbDh9i7p0fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter definition"
      ],
      "metadata": {
        "id": "p2HwKPt7hxf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 6\n",
        "N_SPLITS = 5"
      ],
      "metadata": {
        "id": "KI5cTzkmy3xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')`**: Dòng này xác định thiết bị tính toán mà PyTorch sẽ sử dụng. Nếu CUDA (GPU) khả dụng, thiết bị sẽ được đặt là 'cuda:0' (thiết bị GPU đầu tiên), ngược lại nó sẽ được đặt là 'cpu' (thiết bị CPU).\n",
        "- **`EPOCHS = 6`**: Biến EPOCHS định nghĩa số lượng epochs (vòng lặp huấn luyện) mà mô hình sẽ được huấn luyện qua.\n",
        "- **`N_SPLITS = 5`**: Biến N_SPLITS định nghĩa số lượng phân chia (splits) trong quá trình cross-validation (kiểm định chéo). Đây là số lượng phần chia dữ liệu huấn luyện và kiểm tra mà mô hình sẽ được huấn luyện và đánh giá trên. Trong trường hợp này, có tổng cộng 5 phân chia."
      ],
      "metadata": {
        "id": "jaAEEnC7j-gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load data"
      ],
      "metadata": {
        "id": "1PnysfqhzA2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Introduction"
      ],
      "metadata": {
        "id": "c1dPuAjRvQmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = get_data('/content/drive/MyDrive/Colab Notebooks/PhoBERT_Emotion/UIT-VSMEC/train_nor_811.xlsx')\n",
        "valid_df = get_data('/content/drive/MyDrive/Colab Notebooks/PhoBERT_Emotion/UIT-VSMEC/valid_nor_811.xlsx')\n",
        "test_df = get_data('/content/drive/MyDrive/Colab Notebooks/PhoBERT_Emotion/UIT-VSMEC/test_nor_811.xlsx')"
      ],
      "metadata": {
        "id": "f9PCH5_g1-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS)\n",
        "\n",
        "for fold, (_, val_) in enumerate(skf.split(X=train_df, y=train_df.Emotion)):\n",
        "    train_df.loc[val_, \"kfold\"] = fold"
      ],
      "metadata": {
        "id": "zLWPoBKFlBH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đoạn mã trên thực hiện quá trình chia dữ liệu thành các phần chia kiểm định sử dụng StratifiedKFold.\n",
        "\n",
        "Các bước trong đoạn mã là như sau:\n",
        "\n",
        "1. **`train_df = pd.concat([train_df, valid_df], ignore_index=True)`**: Kết hợp dataframe train_df và valid_df lại thành một dataframe duy nhất. Tham số **`ignore_index=True`** được sử dụng để đặt lại chỉ mục của các dòng trong dataframe kết hợp.\n",
        "2. **`skf = StratifiedKFold(n_splits=N_SPLITS)`**: Khởi tạo một đối tượng StratifiedKFold với số lượng phần chia kiểm định là N_SPLITS.\n",
        "3. **`for fold, (_, val_) in enumerate(skf.split(X=train_df, y=train_df.Emotion))`**: Sử dụng vòng lặp for để duyệt qua các fold được tạo bởi StratifiedKFold. Biến fold đại diện cho số thứ tự của fold, còn (_, val_) đại diện cho chỉ mục của các dữ liệu huấn luyện và dữ liệu kiểm định trong fold hiện tại.\n",
        "4. **`train_df.loc[val_, \"kfold\"] = fold`**: Đánh dấu các dữ liệu có chỉ mục là val_ trong cột \"kfold\" của dataframe train_df với giá trị của fold hiện tại."
      ],
      "metadata": {
        "id": "hDUVw6uAljRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tokenization using PhoBERT"
      ],
      "metadata": {
        "id": "P2g9cLui9c6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
      ],
      "metadata": {
        "id": "I1theKwq9jwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đoạn mã trên sử dụng thư viện **`transformers`** để tải và khởi tạo một tokenizer từ mô hình **`vinai/phobert-base`**.\n",
        "\n",
        "Trong đó:\n",
        "- **`\"vinai/phobert-base\"`** là tên của mô hình PhoBERT được cung cấp bởi VinAI Research.\n",
        "- **`use_fast=False`** được sử dụng để tải tokenizer theo chế độ không sử dụng các cải tiến nhanh của Hugging Face. Mặc định là **`True`**, nhưng khi chọn **`False`**, việc tải tokenizer có thể mất nhiều thời gian hơn nhưng sẽ tiết kiệm bộ nhớ."
      ],
      "metadata": {
        "id": "V6fRnVHdm3Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT works with fixed-length sequences. We’ll use a simple strategy to choose the max length."
      ],
      "metadata": {
        "id": "AY7bUbU59vO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Inference"
      ],
      "metadata": {
        "id": "uV8Em1d5-XFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(f'phobert_fold5.pth', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "IJU3_X8S_U-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Thật là vui vì cuối cùng đã hoàn thành công việc', tokenizer)"
      ],
      "metadata": {
        "id": "0M_ajwtQ-buJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fa87f8-e170-424a-a1dd-c5389c977a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Thật là vui vì cuối cùng đã hoàn thành công việc\n",
            "Sentiment: Enjoyment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Rớt môn rồi, tôi buồn quá', tokenizer)"
      ],
      "metadata": {
        "id": "HH5w3aW5-c1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4f07e1-2405-4785-e750-bba58e4332de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Rớt môn rồi, tôi buồn quá\n",
            "Sentiment: Sadness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Tại sao lại đến trễ, thật bực mình', tokenizer)"
      ],
      "metadata": {
        "id": "sO9tpYKL-elF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ea3d68-e713-4b2b-999c-3d64b7bffb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Tại sao lại đến trễ, thật bực mình\n",
            "Sentiment: Disgust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Ồ! ở đó có nhiều vàng vậy hả?', tokenizer)"
      ],
      "metadata": {
        "id": "GxFG7Ltw-f8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973778d2-5073-4b71-e88a-7856b20f6b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Ồ! ở đó có nhiều vàng vậy hả?\n",
            "Sentiment: Surprise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Cái đó nhìn ghê quá!', tokenizer)"
      ],
      "metadata": {
        "id": "WKKmimAg-g6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Cái đó nhìn ghê quá!', tokenizer)"
      ],
      "metadata": {
        "id": "8rOCNKpL-hzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c08f1db-930c-4653-d7b4-7e3bb105da27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Cái đó nhìn ghê quá!\n",
            "Sentiment: Disgust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Bộ phim này chán quá', tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHzzk346jJ7O",
        "outputId": "0d274f12-8443-406f-bfd9-e7aaac172efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Bộ phim này chán quá\n",
            "Sentiment: Disgust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Em yêu Hệ thống', tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvsJV1MijdiO",
        "outputId": "b63b5119-4b50-4515-bc8f-5aca7dc9dc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Em yêu Hệ thống\n",
            "Sentiment: Enjoyment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer('Tôi không hiểu bộ phim đang nói về vấn đề gì', tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CPRaDuzjjC5",
        "outputId": "1e9287b4-7433-4a61-e029-3aebac3750f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Tôi không hiểu bộ phim đang nói về vấn đề gì\n",
            "Sentiment: Other\n"
          ]
        }
      ]
    }
  ]
}